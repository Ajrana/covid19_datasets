{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "import iris\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import pandas as pd\n",
    "import cftime\n",
    "\n",
    "#Plotting\n",
    "import iris.plot as iplt\n",
    "import iris.quickplot as qplt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#System\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "#Met Office utils\n",
    "import shape_utils as shape\n",
    "\n",
    "#Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Met Office Global Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files for each variable are contained in a separate folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/datasets/open-data/metoffice_global_daily/\n",
      "['precip_max', 'metoffice_orography_global.nc', 'precip_mean', 'sh_max', 'sh_mean', 'sh_min', 'sw_max', 'sw_mean', 't1o5m_max', 't1o5m_mean', 't1o5m_min', 'windgust_max', 'windgust_mean', 'windspeed_max', 'windgust_min', 'windspeed_min', 'windspeed_mean', 'cldbase_max', 'cldbase_mean', 'cldfrac_max', 'cldbase_min', 'cldfrac_mean', 'pmsl_max', 'cldfrac_min', 'pmsl_mean', 'pmsl_min']\n",
      "Number of files for each variable: 209\n"
     ]
    }
   ],
   "source": [
    "#List all the filepaths and store in a dict with each variable as a key\n",
    "\n",
    "folder = '~/datasets/open-data/metoffice_global_daily/'\n",
    "print(os.path.expanduser(folder))\n",
    "filepaths = {path: glob.glob(os.path.join(os.path.expanduser(folder), path, '*.nc')) for path in os.listdir(os.path.expanduser(folder))}\n",
    "variables = list(filepaths.keys())\n",
    "\n",
    "print(variables)\n",
    "print(f'Number of files for each variable: {len(filepaths[variables[0]])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['precip_max', 'precip_mean', 'sh_max', 'sh_mean', 'sh_min', 'sw_max', 'sw_mean', 't1o5m_max', 't1o5m_mean', 't1o5m_min', 'windspeed_max', 'windspeed_min', 'windspeed_mean']\n"
     ]
    }
   ],
   "source": [
    "# FILTERING STEP (!!)\n",
    "# Notebook kernel runs out of memory if we try to process all the cubes at once (for all dates), so here we can subselect from the paths above if necessary - and generate a partial CSV when executing all the steps\n",
    "# In the end we will merge all the partial CSV to produce a full dataset of daily country average for all metrics\n",
    "\n",
    "FILTER_STRING_PRECIPITATION = \"precip_\"\n",
    "FILTER_STRING_HUMIDITY = \"sh_\"\n",
    "FILTER_STRING_SW = \"sw_\"\n",
    "FILTER_STRING_TEMPERATURE = \"t1o5m_\"\n",
    "FILTER_STRING_WINDSPEED = \"windspeed_\"\n",
    "\n",
    "ALL_FILTER_STRINGS = [FILTER_STRING_PRECIPITATION, FILTER_STRING_HUMIDITY, FILTER_STRING_SW, FILTER_STRING_TEMPERATURE, FILTER_STRING_WINDSPEED]\n",
    "\n",
    "#FILTER_STRINGS = [FILTER_STRING_PRECIPITATION]\n",
    "\n",
    "FILTER_STRINGS = ALL_FILTER_STRINGS\n",
    "\n",
    "def has_at_least_one(var, str_list):\n",
    "    for filter_str in str_list:\n",
    "        if filter_str in var:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "filtered_variables = [var for var in variables if has_at_least_one(var, FILTER_STRINGS)]\n",
    "\n",
    "print(filtered_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>city_ascii</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>country</th>\n",
       "      <th>iso2</th>\n",
       "      <th>iso3</th>\n",
       "      <th>admin_name</th>\n",
       "      <th>capital</th>\n",
       "      <th>population</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tokyo</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.6850</td>\n",
       "      <td>139.7514</td>\n",
       "      <td>Japan</td>\n",
       "      <td>JP</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Tōkyō</td>\n",
       "      <td>primary</td>\n",
       "      <td>35676000.0</td>\n",
       "      <td>1392685764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>40.6943</td>\n",
       "      <td>-73.9249</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>New York</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19354922.0</td>\n",
       "      <td>1840034016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mexico City</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>19.4424</td>\n",
       "      <td>-99.1310</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>MX</td>\n",
       "      <td>MEX</td>\n",
       "      <td>Ciudad de México</td>\n",
       "      <td>primary</td>\n",
       "      <td>19028000.0</td>\n",
       "      <td>1484247881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>19.0170</td>\n",
       "      <td>72.8570</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>Mahārāshtra</td>\n",
       "      <td>admin</td>\n",
       "      <td>18978000.0</td>\n",
       "      <td>1356226629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>São Paulo</td>\n",
       "      <td>Sao Paulo</td>\n",
       "      <td>-23.5587</td>\n",
       "      <td>-46.6250</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>BR</td>\n",
       "      <td>BRA</td>\n",
       "      <td>São Paulo</td>\n",
       "      <td>admin</td>\n",
       "      <td>18845000.0</td>\n",
       "      <td>1076532519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>28.6700</td>\n",
       "      <td>77.2300</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>admin</td>\n",
       "      <td>15926000.0</td>\n",
       "      <td>1356872604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shanghai</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>31.2165</td>\n",
       "      <td>121.4365</td>\n",
       "      <td>China</td>\n",
       "      <td>CN</td>\n",
       "      <td>CHN</td>\n",
       "      <td>Shanghai</td>\n",
       "      <td>admin</td>\n",
       "      <td>14987000.0</td>\n",
       "      <td>1156073548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>22.4950</td>\n",
       "      <td>88.3247</td>\n",
       "      <td>India</td>\n",
       "      <td>IN</td>\n",
       "      <td>IND</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>admin</td>\n",
       "      <td>14787000.0</td>\n",
       "      <td>1356060520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>34.1139</td>\n",
       "      <td>-118.4068</td>\n",
       "      <td>United States</td>\n",
       "      <td>US</td>\n",
       "      <td>USA</td>\n",
       "      <td>California</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12815475.0</td>\n",
       "      <td>1840020491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dhaka</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>23.7231</td>\n",
       "      <td>90.4086</td>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>BD</td>\n",
       "      <td>BGD</td>\n",
       "      <td>Dhaka</td>\n",
       "      <td>primary</td>\n",
       "      <td>12797394.0</td>\n",
       "      <td>1050529279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city   city_ascii      lat       lng        country iso2 iso3  \\\n",
       "0        Tokyo        Tokyo  35.6850  139.7514          Japan   JP  JPN   \n",
       "1     New York     New York  40.6943  -73.9249  United States   US  USA   \n",
       "2  Mexico City  Mexico City  19.4424  -99.1310         Mexico   MX  MEX   \n",
       "3       Mumbai       Mumbai  19.0170   72.8570          India   IN  IND   \n",
       "4    São Paulo    Sao Paulo -23.5587  -46.6250         Brazil   BR  BRA   \n",
       "5        Delhi        Delhi  28.6700   77.2300          India   IN  IND   \n",
       "6     Shanghai     Shanghai  31.2165  121.4365          China   CN  CHN   \n",
       "7      Kolkata      Kolkata  22.4950   88.3247          India   IN  IND   \n",
       "8  Los Angeles  Los Angeles  34.1139 -118.4068  United States   US  USA   \n",
       "9        Dhaka        Dhaka  23.7231   90.4086     Bangladesh   BD  BGD   \n",
       "\n",
       "         admin_name  capital  population          id  \n",
       "0             Tōkyō  primary  35676000.0  1392685764  \n",
       "1          New York      NaN  19354922.0  1840034016  \n",
       "2  Ciudad de México  primary  19028000.0  1484247881  \n",
       "3       Mahārāshtra    admin  18978000.0  1356226629  \n",
       "4         São Paulo    admin  18845000.0  1076532519  \n",
       "5             Delhi    admin  15926000.0  1356872604  \n",
       "6          Shanghai    admin  14987000.0  1156073548  \n",
       "7       West Bengal    admin  14787000.0  1356060520  \n",
       "8        California      NaN  12815475.0  1840020491  \n",
       "9             Dhaka  primary  12797394.0  1050529279  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cities lat lon data.\n",
    "\n",
    "cities_path = os.path.expanduser('~/datasets/share/datapoet/cities_latlon_data/worldcities.csv')\n",
    "cities_df = pd.read_csv(cities_path)\n",
    "cities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a dictionary mapping each country to its cities, containing data on latitude, longitude, and population.\n",
    "\n",
    "import collections\n",
    "\n",
    "country_to_city_dict = collections.defaultdict(list)\n",
    "City = collections.namedtuple(\"City\", \"city lat lng population\")\n",
    "\n",
    "for index, row in cities_df.iterrows():\n",
    "    country_to_city_dict[row[\"iso3\"]].append(City(row[\"city\"], row[\"lat\"], row[\"lng\"], row[\"population\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: precipitation_flux / (kg m-2 s-1)   (time: 209; latitude: 1920; longitude: 2560)\n",
      "1: precipitation_flux / (kg m-2 s-1)   (time: 209; latitude: 1920; longitude: 2560)\n",
      "2: specific_humidity / (1)             (time: 209; latitude: 1920; longitude: 2560)\n",
      "3: specific_humidity / (1)             (time: 209; latitude: 1920; longitude: 2560)\n",
      "4: specific_humidity / (1)             (time: 209; latitude: 1920; longitude: 2560)\n",
      "5: m01s01i202 / (1)                    (time: 209; latitude: 1920; longitude: 2560)\n",
      "6: m01s01i202 / (1)                    (time: 209; latitude: 1920; longitude: 2560)\n",
      "7: air_temperature / (K)               (time: 209; latitude: 1920; longitude: 2560)\n",
      "8: air_temperature / (K)               (time: 209; latitude: 1920; longitude: 2560)\n",
      "9: air_temperature / (K)               (time: 209; latitude: 1920; longitude: 2560)\n",
      "10: wind_speed / (m s-1)                (time: 209; latitude: 1921; longitude: 2560)\n",
      "11: wind_speed / (m s-1)                (time: 209; latitude: 1921; longitude: 2560)\n",
      "12: wind_speed / (m s-1)                (time: 209; latitude: 1921; longitude: 2560)\n",
      "CPU times: user 3min 42s, sys: 25 s, total: 4min 7s\n",
      "Wall time: 10min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run through all the variables and append the loaded cubes to a CubeList\n",
    "cubes = iris.cube.CubeList([])\n",
    "\n",
    "for var in filtered_variables:\n",
    "    cubes.extend(iris.load(filepaths[var]))\n",
    "    \n",
    "print(cubes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Precipitation_Weighted_Daily_Average_maximum', 'Precipitation_Weighted_Daily_Average_mean', 'Humidity_Weighted_Daily_Average_maximum', 'Humidity_Weighted_Daily_Average_mean', 'Humidity_Weighted_Daily_Average_minimum', 'SW_Weighted_Daily_Average_maximum', 'SW_Weighted_Daily_Average_mean', 'Temperature_Weighted_Daily_Average_maximum', 'Temperature_Weighted_Daily_Average_mean', 'Temperature_Weighted_Daily_Average_minimum', 'Wind_Speed_Weighted_Daily_Average_maximum', 'Wind_Speed_Weighted_Daily_Average_minimum', 'Wind_Speed_Weighted_Daily_Average_mean']\n"
     ]
    }
   ],
   "source": [
    "# Fetch only the types that we need\n",
    "\n",
    "selected_cubes = iris.cube.CubeList([])\n",
    "selected_cube_derived_stat_names = []\n",
    "\n",
    "for cube in cubes:\n",
    "    if cube.name() == \"air_temperature\":\n",
    "        cube.convert_units('Celsius')\n",
    "        selected_cubes.append(cube)\n",
    "        selected_cube_derived_stat_names.append(\"Temperature_Weighted_Daily_Average_\" + str(cube.cell_methods[0].method))\n",
    "    elif cube.name() == \"specific_humidity\":\n",
    "        selected_cubes.append(cube)\n",
    "        selected_cube_derived_stat_names.append(\"Humidity_Weighted_Daily_Average_\" + str(cube.cell_methods[0].method))\n",
    "    elif cube.name() == \"precipitation_flux\":\n",
    "        selected_cubes.append(cube)\n",
    "        # Note that for precipitation, we index the cell_methods by 1, not 0, due to the initial one referring to a different averaging stage\n",
    "        selected_cube_derived_stat_names.append(\"Precipitation_Weighted_Daily_Average_\" + str(cube.cell_methods[1].method))\n",
    "    elif cube.name() == \"m01s01i202\":\n",
    "        selected_cubes.append(cube)\n",
    "        selected_cube_derived_stat_names.append(\"SW_Weighted_Daily_Average_\" + str(cube.cell_methods[0].method))\n",
    "    elif cube.name() == \"wind_speed\":\n",
    "        selected_cubes.append(cube)\n",
    "        selected_cube_derived_stat_names.append(\"Wind_Speed_Weighted_Daily_Average_\" + str(cube.cell_methods[0].method))\n",
    "        \n",
    "print(selected_cube_derived_stat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n",
      "2020-01-01 11:00:00\n",
      "2020-07-27 11:00:00\n",
      "438299\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import cftime\n",
    "\n",
    "# NOTE: All cubes are aligned in terms of time, so we take the dates from one of them\n",
    "# TODO: add additional logic to support for misaligned cubes, if in the future the assumptions do not hold\n",
    "time_coord_temp = cubes[0].coord(\"time\")\n",
    "all_cube_dates = time_coord_temp.units.num2date(time_coord_temp.points)\n",
    "print(len(all_cube_dates))\n",
    "print(all_cube_dates[0])\n",
    "print(all_cube_dates[-1])\n",
    "\n",
    "print(int(time_coord_temp.points[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-10 12:00:00\n",
      "442884.0\n"
     ]
    }
   ],
   "source": [
    "# We don't need to be processing the entirety of the time range each time, but are rather generating incremental updates - and here we specify the cutoff date, which is the end date of the previous update - we will cut the data along that vertical and only process subsequent dates in the cells that follow.\n",
    "\n",
    "CUTOFF_TIME = cftime.DatetimeGregorian(year=2020, month=7, day=10, hour=12)\n",
    "\n",
    "print(CUTOFF_TIME)\n",
    "print(time_coord_temp.units.date2num(CUTOFF_TIME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation_flux / (kg m-2 s-1)   (time: 209; latitude: 1920; longitude: 2560)\n",
      "     Dimension coordinates:\n",
      "          time                           x              -                -\n",
      "          latitude                       -              x                -\n",
      "          longitude                      -              -                x\n",
      "     Auxiliary coordinates:\n",
      "          forecast_reference_time        x              -                -\n",
      "     Scalar coordinates:\n",
      "          forecast_period: 2.0 hours, bound=(-1.0, 5.0) hours\n",
      "     Attributes:\n",
      "          Conventions: CF-1.5\n",
      "          STASH: m01s05i216\n",
      "          source: Data from Met Office Unified Model\n",
      "          um_version: 11.2\n",
      "     Cell methods:\n",
      "          mean: time (1 hour)\n",
      "          maximum: time\n"
     ]
    }
   ],
   "source": [
    "print(selected_cubes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(selected_cubes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precipitation_flux\n",
      "mean\n",
      "192\n",
      "2020-07-11 11:00:00\n",
      "precipitation_flux\n",
      "mean\n",
      "192\n",
      "2020-07-11 11:00:00\n",
      "specific_humidity\n",
      "maximum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "specific_humidity\n",
      "mean\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "specific_humidity\n",
      "minimum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "m01s01i202\n",
      "maximum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "m01s01i202\n",
      "mean\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "air_temperature\n",
      "maximum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "air_temperature\n",
      "mean\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "air_temperature\n",
      "minimum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "wind_speed\n",
      "maximum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "wind_speed\n",
      "minimum\n",
      "192\n",
      "2020-07-11 11:30:00\n",
      "wind_speed\n",
      "mean\n",
      "192\n",
      "2020-07-11 11:30:00\n"
     ]
    }
   ],
   "source": [
    "# Subsetting to the time range\n",
    "\n",
    "selected_cube_slices = []\n",
    "all_cube_dates_sliced = None\n",
    "\n",
    "for cube in selected_cubes:\n",
    "    time_coord_subsetting = cube.coord(\"time\")\n",
    "    cut_index = 0\n",
    "    cut_time = 0\n",
    "    for i, t in enumerate(time_coord_subsetting.points):\n",
    "        if t > time_coord_subsetting.units.date2num(CUTOFF_TIME):\n",
    "            cut_index = i\n",
    "            cut_time = t\n",
    "            break\n",
    "    print(cube.name())\n",
    "    print(str(cube.cell_methods[0].method))\n",
    "    print(cut_index)\n",
    "    print(time_coord_subsetting.units.num2date(cut_time))\n",
    "    if all_cube_dates_sliced is None:\n",
    "        all_cube_dates_sliced = all_cube_dates[cut_index:]\n",
    "    cube_slice = cube[cut_index:,:,:]\n",
    "    selected_cube_slices.append(cube_slice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIN for Precipitation_Weighted_Daily_Average_maximum\n",
      "0.0\n",
      "MAX for Precipitation_Weighted_Daily_Average_maximum\n",
      "0.051280975\n",
      "MEAN for Precipitation_Weighted_Daily_Average_maximum\n",
      "0.00018588956\n",
      "MIN for Precipitation_Weighted_Daily_Average_mean\n",
      "0.0\n",
      "MAX for Precipitation_Weighted_Daily_Average_mean\n",
      "0.009557724\n",
      "MEAN for Precipitation_Weighted_Daily_Average_mean\n",
      "2.9954384e-05\n",
      "MIN for Humidity_Weighted_Daily_Average_maximum\n",
      "0.0\n",
      "MAX for Humidity_Weighted_Daily_Average_maximum\n",
      "0.061279297\n",
      "MEAN for Humidity_Weighted_Daily_Average_maximum\n",
      "0.0093509145\n",
      "MIN for Humidity_Weighted_Daily_Average_mean\n",
      "0.0\n",
      "MAX for Humidity_Weighted_Daily_Average_mean\n",
      "0.028645834\n",
      "MEAN for Humidity_Weighted_Daily_Average_mean\n",
      "0.008464838\n",
      "MIN for Humidity_Weighted_Daily_Average_minimum\n",
      "0.0\n",
      "MAX for Humidity_Weighted_Daily_Average_minimum\n",
      "0.02758789\n",
      "MEAN for Humidity_Weighted_Daily_Average_minimum\n",
      "0.0077401577\n",
      "MIN for SW_Weighted_Daily_Average_maximum\n",
      "0.0\n",
      "MAX for SW_Weighted_Daily_Average_maximum\n",
      "1079.0\n",
      "MEAN for SW_Weighted_Daily_Average_maximum\n",
      "467.38647\n",
      "MIN for SW_Weighted_Daily_Average_mean\n",
      "0.0\n",
      "MAX for SW_Weighted_Daily_Average_mean\n",
      "368.4694\n",
      "MEAN for SW_Weighted_Daily_Average_mean\n",
      "136.82652\n",
      "MIN for Temperature_Weighted_Daily_Average_maximum\n",
      "-84.15\n",
      "MAX for Temperature_Weighted_Daily_Average_maximum\n",
      "52.35\n",
      "MEAN for Temperature_Weighted_Daily_Average_maximum\n",
      "10.517934\n",
      "MIN for Temperature_Weighted_Daily_Average_mean\n",
      "-85.83294\n",
      "MAX for Temperature_Weighted_Daily_Average_mean\n",
      "44.273827\n",
      "MEAN for Temperature_Weighted_Daily_Average_mean\n",
      "8.374709\n",
      "MIN for Temperature_Weighted_Daily_Average_minimum\n",
      "-89.04063\n",
      "MAX for Temperature_Weighted_Daily_Average_minimum\n",
      "37.959373\n",
      "MEAN for Temperature_Weighted_Daily_Average_minimum\n",
      "6.2181377\n",
      "MIN for Wind_Speed_Weighted_Daily_Average_maximum\n",
      "0.25\n",
      "MAX for Wind_Speed_Weighted_Daily_Average_maximum\n",
      "42.25\n",
      "MEAN for Wind_Speed_Weighted_Daily_Average_maximum\n",
      "8.677173\n",
      "MIN for Wind_Speed_Weighted_Daily_Average_minimum\n",
      "0.0\n",
      "MAX for Wind_Speed_Weighted_Daily_Average_minimum\n",
      "27.875\n",
      "MEAN for Wind_Speed_Weighted_Daily_Average_minimum\n",
      "4.494438\n",
      "MIN for Wind_Speed_Weighted_Daily_Average_mean\n",
      "0.119791664\n",
      "MAX for Wind_Speed_Weighted_Daily_Average_mean\n",
      "32.786457\n",
      "MEAN for Wind_Speed_Weighted_Daily_Average_mean\n",
      "6.601946\n"
     ]
    }
   ],
   "source": [
    "# Optionally, we can look into the stats across the cubes, for the date range selected - to see if everything seems reasonable.\n",
    "\n",
    "for cube, name in zip(selected_cube_slices, selected_cube_derived_stat_names):\n",
    "    print(\"MIN for \" + name)\n",
    "    print(cube.data.min())\n",
    "    print(\"MAX for \" + name)\n",
    "    print(cube.data.max())\n",
    "    print(\"MEAN for \" + name)\n",
    "    print(cube.data.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting code, given a sampled cube / after a query, for data visualization\n",
    "\n",
    "#import numpy as np\n",
    "#import matplotlib as plt\n",
    "\n",
    "#hist, bins = np.histogram(ex_cube.data, bins=20, range=(-20, 20), weights=None, density=True)\n",
    "#hist = hist / np.sum(hist)\n",
    "#plt.pyplot.bar(bins[:-1], hist,  width=(bins[1] - bins[0]))\n",
    "#plt.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Precipitation_Weighted_Daily_Average_maximum\n",
      "Processing Precipitation_Weighted_Daily_Average_mean\n",
      "Processing Humidity_Weighted_Daily_Average_maximum\n",
      "Processing Humidity_Weighted_Daily_Average_mean\n",
      "Processing Humidity_Weighted_Daily_Average_minimum\n",
      "Processing SW_Weighted_Daily_Average_maximum\n",
      "Processing SW_Weighted_Daily_Average_mean\n",
      "Processing Temperature_Weighted_Daily_Average_maximum\n",
      "Processing Temperature_Weighted_Daily_Average_mean\n",
      "Processing Temperature_Weighted_Daily_Average_minimum\n",
      "Processing Wind_Speed_Weighted_Daily_Average_maximum\n",
      "Processing Wind_Speed_Weighted_Daily_Average_minimum\n",
      "Processing Wind_Speed_Weighted_Daily_Average_mean\n"
     ]
    }
   ],
   "source": [
    "# For each of the metrics, we compute a dictionary from a country code into all of the cubes corresponding to results of lat/long queries for cities from that country\n",
    "# The ordering of the dicts in the list is the same as the ordering of the original cubes and their name list\n",
    "\n",
    "LATLONG_DELTA = 0.2 # Degrees, defining the 'vicinity of a city' as lat/long +- LATLONG_DELTA\n",
    "\n",
    "all_country_city_cubes_dicts = []\n",
    "\n",
    "for cube, name in zip(selected_cube_slices, selected_cube_derived_stat_names):\n",
    "    print(\"Processing \" + name)\n",
    "    country_city_cubes_dict = collections.defaultdict(list)\n",
    "    for iso_code in country_to_city_dict.keys():\n",
    "        for city in country_to_city_dict[iso_code]:\n",
    "            city_latlon = ((city.lat - LATLONG_DELTA, city.lat + LATLONG_DELTA), (city.lng - LATLONG_DELTA, city.lng + LATLONG_DELTA))\n",
    "            city_cube = cube.intersection(latitude=city_latlon[0], longitude=city_latlon[1])\n",
    "            country_city_cubes_dict[iso_code].append(city_cube)\n",
    "    all_country_city_cubes_dicts.append(country_city_cubes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Precipitation_Weighted_Daily_Average_maximum\n",
      "Processing Precipitation_Weighted_Daily_Average_mean\n",
      "Processing Humidity_Weighted_Daily_Average_maximum\n",
      "Processing Humidity_Weighted_Daily_Average_mean\n",
      "Processing Humidity_Weighted_Daily_Average_minimum\n",
      "Processing SW_Weighted_Daily_Average_maximum\n",
      "Processing SW_Weighted_Daily_Average_mean\n",
      "Processing Temperature_Weighted_Daily_Average_maximum\n",
      "Processing Temperature_Weighted_Daily_Average_mean\n",
      "Processing Temperature_Weighted_Daily_Average_minimum\n",
      "Processing Wind_Speed_Weighted_Daily_Average_maximum\n",
      "Processing Wind_Speed_Weighted_Daily_Average_minimum\n",
      "Processing Wind_Speed_Weighted_Daily_Average_mean\n"
     ]
    }
   ],
   "source": [
    "# Here we compute the population-weighted averages for all metrics.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "all_country_averages_dicts = []\n",
    "\n",
    "for cube, name, city_cubes_dict in zip(selected_cube_slices, selected_cube_derived_stat_names, all_country_city_cubes_dicts):\n",
    "    print(\"Processing \" + name)\n",
    "    country_averages_dict = collections.defaultdict(list)\n",
    "    \n",
    "    # We compute the weights for the convex combination, from the city populations.\n",
    "    for iso_code in country_to_city_dict.keys():\n",
    "        weights = []\n",
    "        for city in country_to_city_dict[iso_code]:\n",
    "            if np.isnan(city.population):\n",
    "                weights.append(0.)\n",
    "            else:\n",
    "                weights.append(float(city.population))\n",
    "        # Normalization, so that they sum up to 1.\n",
    "        weights = np.asarray(weights)\n",
    "        weights /= np.sum(weights)\n",
    "\n",
    "        # Computation of daily averages\n",
    "        for date_index in range(len(all_cube_dates_sliced)):\n",
    "            value_list = []\n",
    "            for city_cube in city_cubes_dict[iso_code]:\n",
    "                value_list.append(np.mean(city_cube.data[date_index, :, :]))\n",
    "            masked_values = np.ma.MaskedArray(value_list, mask=np.isnan(value_list))\n",
    "            country_weighted_day_mean = np.average(masked_values, weights=weights)\n",
    "            country_averages_dict[iso_code].append(country_weighted_day_mean)\n",
    "    all_country_averages_dicts.append(country_averages_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes we need to generate data in multiple runs, or incrementally generate updates, due to container memory restrictions.\n",
    "# This could lead to multiple generated CSV-s with the split being either by time or by data type.\n",
    "# Here we generate the update from the current sub-selection, which will then be merged with the rest of the data in the subsequent step.\n",
    "\n",
    "FILE_SUFFIX = \"_update_6\"\n",
    "\n",
    "iso_column_vals = []\n",
    "date_column_vals = []\n",
    "\n",
    "selected_cube_derived_stat_names\n",
    "\n",
    "all_features_column_vals = [[] for _ in range(len(selected_cube_derived_stat_names))]\n",
    "\n",
    "for iso_code in country_to_city_dict.keys():\n",
    "    for date_index in range(len(all_cube_dates_sliced)):\n",
    "        iso_column_vals.append(iso_code)\n",
    "        date_column_vals.append(all_cube_dates_sliced[date_index].strftime(\"%Y-%m-%d\"))\n",
    "        for val_list, country_averages_dict in zip(all_features_column_vals, all_country_averages_dicts):\n",
    "            val_list.append(country_averages_dict[iso_code][date_index])\n",
    "\n",
    "data = {\n",
    "        'ISO':  iso_column_vals,\n",
    "        'Date':  date_column_vals}\n",
    "\n",
    "for name, vals in zip(selected_cube_derived_stat_names, all_features_column_vals):\n",
    "    data[name] = vals\n",
    "    \n",
    "column_names = ['ISO', 'Date']\n",
    "column_names.extend(selected_cube_derived_stat_names)\n",
    "\n",
    "export_df = pd.DataFrame (data, columns = column_names)\n",
    "\n",
    "out_path = os.path.expanduser('~/datasets/share/datapoet/weather_summaries/countries_daily_weighted_averages' + FILE_SUFFIX + '.csv')\n",
    "export_df.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can potentially join the multiple individual CSV-s into a joint one - if we have been processing different quan\n",
    "\n",
    "#output_folder = '~/datasets/share/datapoet/weather_summaries/'\n",
    "\n",
    "#print(os.path.expanduser(output_folder))\n",
    "#filepaths = glob.glob(os.path.join(os.path.expanduser(output_folder), 'countries_daily_weighted_averages*'))\n",
    "\n",
    "#print(filepaths)\n",
    "\n",
    "#all_dfs = []\n",
    "\n",
    "#for path in filepaths:\n",
    "#    df = pd.read_csv(path)\n",
    "#    all_dfs.append(df)\n",
    "    \n",
    "#print(len(all_dfs))\n",
    "\n",
    "#merge_keys = ['ISO', 'Date']\n",
    "\n",
    "#merged_df = all_dfs[0]\n",
    "\n",
    "#for i in range(1, len(all_dfs)):\n",
    "#    print(\"Merged with \" + str(i))\n",
    "#    merged_df = pd.merge(merged_df, all_dfs[i], how='inner', on=merge_keys, left_on=None, right_on=None,\n",
    "#                         left_index=False, right_index=False, sort=True,\n",
    "#                         suffixes=('_x', '_y'), copy=True, indicator=False,\n",
    "#                         validate=None)\n",
    "    \n",
    "#merged_out_path = os.path.expanduser('~/datasets/share/datapoet/weather_summaries/countries_daily_weighted_averages_merged.csv')\n",
    "#merged_df.to_csv(merged_out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41924\n",
      "3791\n",
      "45715\n"
     ]
    }
   ],
   "source": [
    "# Or potentially apply an update\n",
    "\n",
    "original_path = os.path.expanduser('~/datasets/share/datapoet/weather_summaries/countries_daily_weighted_averages_july_10.csv')\n",
    "update_path = os.path.expanduser('~/datasets/share/datapoet/weather_summaries/countries_daily_weighted_averages_update_6.csv')\n",
    "\n",
    "original_df = pd.read_csv(original_path)\n",
    "update_df = pd.read_csv(update_path)\n",
    "\n",
    "final_df = original_df.copy()\n",
    "final_df = final_df.append(update_df, sort=True)\n",
    "\n",
    "print(len(original_df))\n",
    "print(len(update_df))\n",
    "print(len(final_df))\n",
    "\n",
    "final_df = final_df.sort_values(by=['ISO', 'Date'])\n",
    "\n",
    "final_out_path = os.path.expanduser('~/datasets/share/datapoet/weather_summaries/countries_daily_weighted_averages_july_27.csv')\n",
    "final_df.to_csv(final_out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
